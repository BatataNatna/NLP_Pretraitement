{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraitement des données avant de passer au NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize  import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rymkm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rymkm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rymkm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer => vectorizer => transformer en vecteurs numerique\n",
    "nltk.download('punkt')\n",
    "#Stopwords => mots à supprimer\n",
    "nltk.download('stopwords')\n",
    "#Lemmatizer => se remettre au radical\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z13lgffb5w3ddx1ul22qy1wxspy5cpkz504</td>\n",
       "      <td>dharma pal</td>\n",
       "      <td>2015-05-29T02:30:18.971000</td>\n",
       "      <td>Nice song﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z123dbgb0mqjfxbtz22ucjc5jvzcv3ykj</td>\n",
       "      <td>Tiza Arellano</td>\n",
       "      <td>2015-05-29T00:14:48.748000</td>\n",
       "      <td>I love song ﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z12quxxp2vutflkxv04cihggzt2azl34pms0k</td>\n",
       "      <td>Prìñçeśś Âliś Łøvê Dømíñø Mâđiś™ ﻿</td>\n",
       "      <td>2015-05-28T21:00:08.607000</td>\n",
       "      <td>I love song ﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z12icv3ysqvlwth2c23eddlykyqut5z1h</td>\n",
       "      <td>Eric Gonzalez</td>\n",
       "      <td>2015-05-28T20:47:12.193000</td>\n",
       "      <td>860,000,000 lets make it first female to reach...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z133stly3kete3tly22petvwdpmghrlli</td>\n",
       "      <td>Analena López</td>\n",
       "      <td>2015-05-28T17:08:29.827000</td>\n",
       "      <td>shakira is best for worldcup﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13T13:27:39.441000</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13T13:14:30.021000</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>_2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs</td>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>2013-07-13T12:09:31.188000</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13T11:17:52.308000</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12T22:33:27.916000</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMMENT_ID  \\\n",
       "0            z13lgffb5w3ddx1ul22qy1wxspy5cpkz504   \n",
       "1              z123dbgb0mqjfxbtz22ucjc5jvzcv3ykj   \n",
       "2          z12quxxp2vutflkxv04cihggzt2azl34pms0k   \n",
       "3              z12icv3ysqvlwth2c23eddlykyqut5z1h   \n",
       "4              z133stly3kete3tly22petvwdpmghrlli   \n",
       "..                                           ...   \n",
       "365  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA   \n",
       "366  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI   \n",
       "367  _2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs   \n",
       "368  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0   \n",
       "369  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA   \n",
       "\n",
       "                                 AUTHOR                        DATE  \\\n",
       "0                            dharma pal  2015-05-29T02:30:18.971000   \n",
       "1                         Tiza Arellano  2015-05-29T00:14:48.748000   \n",
       "2    Prìñçeśś Âliś Łøvê Dømíñø Mâđiś™ ﻿  2015-05-28T21:00:08.607000   \n",
       "3                         Eric Gonzalez  2015-05-28T20:47:12.193000   \n",
       "4                         Analena López  2015-05-28T17:08:29.827000   \n",
       "..                                  ...                         ...   \n",
       "365                        Katie Mettam  2013-07-13T13:27:39.441000   \n",
       "366                Sabina Pearson-Smith  2013-07-13T13:14:30.021000   \n",
       "367                       jeffrey jules  2013-07-13T12:09:31.188000   \n",
       "368                      Aishlin Maciel  2013-07-13T11:17:52.308000   \n",
       "369                         Latin Bosch  2013-07-12T22:33:27.916000   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "0                                           Nice song﻿      0  \n",
       "1                                        I love song ﻿      0  \n",
       "2                                        I love song ﻿      0  \n",
       "3    860,000,000 lets make it first female to reach...      0  \n",
       "4                        shakira is best for worldcup﻿      0  \n",
       "..                                                 ...    ...  \n",
       "365  I love this song because we sing it at Camp al...      0  \n",
       "366  I love this song for two reasons: 1.it is abou...      0  \n",
       "367                                                wow      0  \n",
       "368                            Shakira u are so wiredo      0  \n",
       "369                         Shakira is the best dancer      0  \n",
       "\n",
       "[370 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Youtube05-Shakira.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netooyer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supprimer les lignes \n",
    "df =df.dropna(subset=['CONTENT'])\n",
    "#Supprimer les doublons\n",
    "df = df.drop_duplicates(subset=['CONTENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction pour tokenizer, enlever les stopwords, lemmatizer ...etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour nettoyer, tokenizer, lemmatiser  et stemmer\n",
    "def process_text(text):\n",
    "    #convertir en minuscules\n",
    "    text = text.lower()\n",
    "    #Supprimer les caractères non alphabétiques\n",
    "    text=re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "    #Enlever html\n",
    "    text=re.sub(r'\\d+','',text)\n",
    "    #supprimer la ponctuation\n",
    "    text=re.sub(r'[^\\w\\s]', '', text)\n",
    "    #Tokenisation avec NLTK\n",
    "    tokens= word_tokenize(text)\n",
    "    #supprimer les stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if word not in stop_words]\n",
    "    #lemmatisation avec spacy\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized_tokens=[token.lemma_ for token in doc]\n",
    "    #stemming \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmer_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    #Rejoindre les tokens en une chaine\n",
    "    processed_text = ''.join(stemmer_tokens)\n",
    "    return  processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cerner la colonne à cleaner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['CONTENT']\n",
    "#Appliquer la fonction à cette colonne\n",
    "for column in columns_to_clean:\n",
    "    df[column + '_processed']= df[column].apply(process_text)\n",
    "\n",
    "#Exporter les données clean en csv sans les indices\n",
    "df.to_csv(\"SHAKIRA1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =pd.read_csv(\"SHAKIRA1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>CONTENT_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>z12uujnj2sifvzvav04chpypvofvexpoggg</td>\n",
       "      <td>Sudheer Yadav</td>\n",
       "      <td>2015-05-28T10:28:25.133000</td>\n",
       "      <td>SEE SOME MORE SONG OPEN GOOGLE AND TYPE Shakir...</td>\n",
       "      <td>1</td>\n",
       "      <td>seesongopengoogltypeshakiraguruofmovi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>z13zjlpo2nbehxwf322gelhzwmqwgn1mt</td>\n",
       "      <td>Raafat saeed</td>\n",
       "      <td>2015-05-27T04:19:29.178000</td>\n",
       "      <td>Check out this playlist on YouTube:﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>checkplaylistyoutub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>z13uhhxp5nvig15yc04citszvtagwtmpqcc</td>\n",
       "      <td>Terry Short</td>\n",
       "      <td>2015-05-26T14:33:52.496000</td>\n",
       "      <td>Support the fight for your 4th amendment right...</td>\n",
       "      <td>1</td>\n",
       "      <td>supportfightthamendrightprivacihomestopnsaspia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>z13gv1bxbuytgjl3o23fdr5r3kaadbbm1</td>\n",
       "      <td>‫حلم الشباب‬‎</td>\n",
       "      <td>2015-05-25T23:42:49.533000</td>\n",
       "      <td>Check out this video on YouTube:﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>checkvideoyoutub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>z12bfraboyajftgbz04ccbkr3xjxfxyxsew</td>\n",
       "      <td>Abdullah Fawzi</td>\n",
       "      <td>2015-05-25T06:25:22.319000</td>\n",
       "      <td>coby this USL and past :&lt;br /&gt;&lt;a href=\"http://...</td>\n",
       "      <td>1</td>\n",
       "      <td>cobiuslpastbrhrefhttpadflyhttpadflyahmvtxbrdel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>z12xc3ly4x3uttmci22xff24nqqxwb0je04</td>\n",
       "      <td>Lisa Matthews</td>\n",
       "      <td>2013-07-17T13:56:03.233000</td>\n",
       "      <td>Check out this video on YouTube:&lt;br /&gt;&amp;quot;Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>checkvideoyoutubebrquotthitimeafricaquotonetra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>_2viQ_Qnc69GH3FQl348HonbRxpbmtsR5CUei0zkJog</td>\n",
       "      <td>Riley Rollins</td>\n",
       "      <td>2013-07-16T00:30:46.660000</td>\n",
       "      <td>O peoples of the earth, I have seen how you pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>peoplearthseeperformeveriformevilleisurceasrev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>_2viQ_Qnc6-qHJ_u9Yv84vj4yOAPLUL3ZibCc7b-vBI</td>\n",
       "      <td>FAHAD KHAN</td>\n",
       "      <td>2013-07-14T22:06:57.712000</td>\n",
       "      <td>I WILL NEVER FORGET THIS SONG IN MY LIFE LIKE ...</td>\n",
       "      <td>1</td>\n",
       "      <td>neverforgetsonglifelikecommenthearsonglikeyear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>_2viQ_Qnc6_HU65mTzCmXnjA-WLt7XqxqPj7EwAtlO0</td>\n",
       "      <td>ricky swaggz</td>\n",
       "      <td>2013-07-14T20:40:00.331000</td>\n",
       "      <td>********OMG Facebook is OLD! Check out  ------...</td>\n",
       "      <td>1</td>\n",
       "      <td>omgfacebookoldcheckgtswagfriendcommakethousand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>_2viQ_Qnc6_fgKR1W7-k1lbVURi8hVbMlQAMSOCSnyk</td>\n",
       "      <td>ThirdDegr3e</td>\n",
       "      <td>2013-07-13T20:48:22.967000</td>\n",
       "      <td>**CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...</td>\n",
       "      <td>1</td>\n",
       "      <td>checknewmixtapchecknewmixtapchecknewmixtapchec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMMENT_ID          AUTHOR  \\\n",
       "6            z12uujnj2sifvzvav04chpypvofvexpoggg   Sudheer Yadav   \n",
       "21             z13zjlpo2nbehxwf322gelhzwmqwgn1mt    Raafat saeed   \n",
       "29           z13uhhxp5nvig15yc04citszvtagwtmpqcc     Terry Short   \n",
       "34             z13gv1bxbuytgjl3o23fdr5r3kaadbbm1   ‫حلم الشباب‬‎   \n",
       "49           z12bfraboyajftgbz04ccbkr3xjxfxyxsew  Abdullah Fawzi   \n",
       "..                                           ...             ...   \n",
       "316          z12xc3ly4x3uttmci22xff24nqqxwb0je04   Lisa Matthews   \n",
       "318  _2viQ_Qnc69GH3FQl348HonbRxpbmtsR5CUei0zkJog   Riley Rollins   \n",
       "321  _2viQ_Qnc6-qHJ_u9Yv84vj4yOAPLUL3ZibCc7b-vBI      FAHAD KHAN   \n",
       "322  _2viQ_Qnc6_HU65mTzCmXnjA-WLt7XqxqPj7EwAtlO0    ricky swaggz   \n",
       "323  _2viQ_Qnc6_fgKR1W7-k1lbVURi8hVbMlQAMSOCSnyk     ThirdDegr3e   \n",
       "\n",
       "                           DATE  \\\n",
       "6    2015-05-28T10:28:25.133000   \n",
       "21   2015-05-27T04:19:29.178000   \n",
       "29   2015-05-26T14:33:52.496000   \n",
       "34   2015-05-25T23:42:49.533000   \n",
       "49   2015-05-25T06:25:22.319000   \n",
       "..                          ...   \n",
       "316  2013-07-17T13:56:03.233000   \n",
       "318  2013-07-16T00:30:46.660000   \n",
       "321  2013-07-14T22:06:57.712000   \n",
       "322  2013-07-14T20:40:00.331000   \n",
       "323  2013-07-13T20:48:22.967000   \n",
       "\n",
       "                                               CONTENT  CLASS  \\\n",
       "6    SEE SOME MORE SONG OPEN GOOGLE AND TYPE Shakir...      1   \n",
       "21                Check out this playlist on YouTube:﻿      1   \n",
       "29   Support the fight for your 4th amendment right...      1   \n",
       "34                   Check out this video on YouTube:﻿      1   \n",
       "49   coby this USL and past :<br /><a href=\"http://...      1   \n",
       "..                                                 ...    ...   \n",
       "316  Check out this video on YouTube:<br />&quot;Th...      1   \n",
       "318  O peoples of the earth, I have seen how you pe...      1   \n",
       "321  I WILL NEVER FORGET THIS SONG IN MY LIFE LIKE ...      1   \n",
       "322  ********OMG Facebook is OLD! Check out  ------...      1   \n",
       "323  **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...      1   \n",
       "\n",
       "                                     CONTENT_processed  \n",
       "6                seesongopengoogltypeshakiraguruofmovi  \n",
       "21                                 checkplaylistyoutub  \n",
       "29   supportfightthamendrightprivacihomestopnsaspia...  \n",
       "34                                    checkvideoyoutub  \n",
       "49   cobiuslpastbrhrefhttpadflyhttpadflyahmvtxbrdel...  \n",
       "..                                                 ...  \n",
       "316  checkvideoyoutubebrquotthitimeafricaquotonetra...  \n",
       "318  peoplearthseeperformeveriformevilleisurceasrev...  \n",
       "321     neverforgetsonglifelikecommenthearsonglikeyear  \n",
       "322  omgfacebookoldcheckgtswagfriendcommakethousand...  \n",
       "323  checknewmixtapchecknewmixtapchecknewmixtapchec...  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2[df2['CLASS']==1]\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des deux datasets (ancien et le clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 6)\n"
     ]
    }
   ],
   "source": [
    "shape_original = df.shape\n",
    "print(shape_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6)\n"
     ]
    }
   ],
   "source": [
    "shape_clean = df3.shape\n",
    "print(shape_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On remarque le nombre de lignes a diminué (sans les doublons et les nan)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On peut calculer la distance pour avoir des information sur la similarité entre les data clean et brutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn\n",
    "from  sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coller les données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le calcul de similarité/distance entre les mots ou les phrases est appelé \"embeding\" ce qui nous interesse (vu que nous ne pouvons pas calculer deux à deux les distances par faute de tailles différentes), j'ai décidé de coller toutes les lignes des données brutes dans un seul corpus (fichier text: texte_brut.txt) ainsi que les données traitées (texte_clean.txt) puis de calculer la distance entre ces deux longs textes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle pour chaque colonne du DataFrame\n",
    "columns = ['CONTENT','CONTENT_processed']\n",
    "for column in columns:\n",
    "    # Créer un fichier texte avec le nom de la colonne\n",
    "    with open(f\"{column}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        # Parcourir chaque ligne de la colonne et écrire dans le fichier texte\n",
    "        for index, content in enumerate(df[column]):\n",
    "            file.write(f\"{content};\")  # Écrire le contenu de la cellule suivie d'un ;\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaration du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mean_embedding(sentence):\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner la moyenne des vecteurs pour chaque phrase\n",
    "    return np.mean([(X.vector) for X in doc], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norme de la différence entre les embeddings moyens : [0.5419189  0.8843663  0.6116699  0.12794846 1.2093133  0.5381026\n",
      " 0.5730413  0.4932996  0.34796438 0.32431698 0.4286716  0.34020606\n",
      " 0.29290262 1.8185406  0.07552385 0.26419497 0.55989933 0.5810716\n",
      " 0.05225021 0.73851204 1.2131802  0.13992997 0.856899   0.69117785\n",
      " 1.1284418  0.5789551  0.7248291  1.7483066  2.2017727  0.2586306\n",
      " 0.13390177 0.5417625  1.3393643  0.30498374 1.0704259  2.7259994\n",
      " 0.529492   0.01907951 0.5378912  0.5766442  0.11752909 0.62112427\n",
      " 0.6905458  0.7673179  0.12604424 0.10571358 0.36316493 0.61116135\n",
      " 0.5140164  0.20727712 1.6142906  1.5629784  0.9321287  2.0848758\n",
      " 0.5804235  1.4757789  1.8561186  0.76547533 0.20361494 0.15530288\n",
      " 0.22224915 1.2660216  0.8153252  0.52127254 1.0414953  0.19238096\n",
      " 0.5354439  0.2624299  2.192708   0.54946357 0.03865526 0.20413697\n",
      " 0.95211446 0.48590666 1.3335615  0.66200286 0.63879925 1.0570954\n",
      " 0.4561501  1.1280744  0.07156456 0.24075049 0.11465693 0.9302643\n",
      " 0.68942463 0.9370067  0.19693944 1.2496402  0.10106596 0.66775256\n",
      " 0.28621894 0.0822501  0.16788459 0.167256   0.28143674 0.6759393 ]\n"
     ]
    }
   ],
   "source": [
    "# Calcul des embeddings moyens des données brutes\n",
    "sentences_brut = pd.read_csv('CONTENT.txt', header=None, delimiter= ';',encoding='utf-8')\n",
    "embedding_brut = sentences_brut[0].apply(return_mean_embedding)\n",
    "\n",
    "# Calcul des embeddings moyens des données nettoyées\n",
    "sentences_clean = pd.read_csv('CONTENT_processed.txt', header=None, delimiter=';', encoding='utf-8')\n",
    "embedding_clean = sentences_clean[0].apply(return_mean_embedding)\n",
    "\n",
    "# Calcul de la norme de la différence (distance) entre les embeddings\n",
    "norm_diff_2 = np.linalg.norm(embedding_brut - embedding_clean)\n",
    "print(\"Norme de la différence entre les embeddings moyens :\", norm_diff_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nous pouvons donner un seuil à partir duquel on supprime les données processed trop eloignée contextuellement des données brutes**\n",
    "\n",
    "**vous pouvez changer le seuil pour plus ou moins de liberté**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>CONTENT_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z12gddhblwz3cf3wc04cgvjajvulwxvb5lw0k</td>\n",
       "      <td>Dr.geetanjali sharma</td>\n",
       "      <td>2015-05-27T09:14:14.641000</td>\n",
       "      <td>Waka best one﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>wakawellone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z12afj2p2pniy1idm23wclt4ox2wujzj004</td>\n",
       "      <td>Ernest Foli</td>\n",
       "      <td>2015-05-25T21:08:10.798000</td>\n",
       "      <td>fave song﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>favesong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z13iuzxghsypjtfwy04ccnewbqbde3qwea0</td>\n",
       "      <td>Сергей Андреевич</td>\n",
       "      <td>2015-05-25T18:00:22.486000</td>\n",
       "      <td>love Shakira!﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>loveshakira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13ijbpyhnvtddsue223c3pgilfjs3pmv</td>\n",
       "      <td>Ibtihal Baho</td>\n",
       "      <td>2015-05-25T11:21:16.857000</td>\n",
       "      <td>goood﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>goood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z12uvzpw4tufgf2qw04chd5abu3uvf4rw5o</td>\n",
       "      <td>Eric Gonzalez</td>\n",
       "      <td>2015-05-24T15:35:03.844000</td>\n",
       "      <td>To help shakira become the first female to hit...</td>\n",
       "      <td>0</td>\n",
       "      <td>helpshakirabecomfirstfemalhitbilliomviewivedec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              COMMENT_ID                AUTHOR  \\\n",
       "0  z12gddhblwz3cf3wc04cgvjajvulwxvb5lw0k  Dr.geetanjali sharma   \n",
       "1    z12afj2p2pniy1idm23wclt4ox2wujzj004           Ernest Foli   \n",
       "2    z13iuzxghsypjtfwy04ccnewbqbde3qwea0      Сергей Андреевич   \n",
       "3      z13ijbpyhnvtddsue223c3pgilfjs3pmv          Ibtihal Baho   \n",
       "4    z12uvzpw4tufgf2qw04chd5abu3uvf4rw5o         Eric Gonzalez   \n",
       "\n",
       "                         DATE  \\\n",
       "0  2015-05-27T09:14:14.641000   \n",
       "1  2015-05-25T21:08:10.798000   \n",
       "2  2015-05-25T18:00:22.486000   \n",
       "3  2015-05-25T11:21:16.857000   \n",
       "4  2015-05-24T15:35:03.844000   \n",
       "\n",
       "                                             CONTENT  CLASS  \\\n",
       "0                                     Waka best one﻿      0   \n",
       "1                                         fave song﻿      0   \n",
       "2                                     love Shakira!﻿      0   \n",
       "3                                             goood﻿      0   \n",
       "4  To help shakira become the first female to hit...      0   \n",
       "\n",
       "                                   CONTENT_processed  \n",
       "0                                        wakawellone  \n",
       "1                                           favesong  \n",
       "2                                        loveshakira  \n",
       "3                                              goood  \n",
       "4  helpshakirabecomfirstfemalhitbilliomviewivedec...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil = 0.5\n",
    "lignes_supprime = []\n",
    "\n",
    "# Parcourir les distances\n",
    "for i, distance in enumerate(norm_diff_2 ):\n",
    "    if distance > 0.5:\n",
    "        lignes_supprime.append(i)\n",
    "\n",
    "# Supprimer les lignes correspondant aux index des distances supérieures à 0.5\n",
    "df2.drop(lignes_supprime, inplace=True)\n",
    "\n",
    "# Réinitialiser les index après la suppression\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "df2.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** : df2 est maintenant prete à être utilisée telle qu'elle pour l'application des models de NLP sans être biaisé par le résultat duc leanning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
