{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraitement des données avant de passer au NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize  import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rymkm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rymkm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rymkm\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer => vectorizer => transformer en vecteurs numerique\n",
    "nltk.download('punkt')\n",
    "#Stopwords => mots à supprimer\n",
    "nltk.download('stopwords')\n",
    "#Lemmatizer => se remettre au radical\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z13lgffb5w3ddx1ul22qy1wxspy5cpkz504</td>\n",
       "      <td>dharma pal</td>\n",
       "      <td>2015-05-29T02:30:18.971000</td>\n",
       "      <td>Nice song﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z123dbgb0mqjfxbtz22ucjc5jvzcv3ykj</td>\n",
       "      <td>Tiza Arellano</td>\n",
       "      <td>2015-05-29T00:14:48.748000</td>\n",
       "      <td>I love song ﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z12quxxp2vutflkxv04cihggzt2azl34pms0k</td>\n",
       "      <td>Prìñçeśś Âliś Łøvê Dømíñø Mâđiś™ ﻿</td>\n",
       "      <td>2015-05-28T21:00:08.607000</td>\n",
       "      <td>I love song ﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z12icv3ysqvlwth2c23eddlykyqut5z1h</td>\n",
       "      <td>Eric Gonzalez</td>\n",
       "      <td>2015-05-28T20:47:12.193000</td>\n",
       "      <td>860,000,000 lets make it first female to reach...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z133stly3kete3tly22petvwdpmghrlli</td>\n",
       "      <td>Analena López</td>\n",
       "      <td>2015-05-28T17:08:29.827000</td>\n",
       "      <td>shakira is best for worldcup﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13T13:27:39.441000</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13T13:14:30.021000</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>_2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs</td>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>2013-07-13T12:09:31.188000</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13T11:17:52.308000</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12T22:33:27.916000</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMMENT_ID  \\\n",
       "0            z13lgffb5w3ddx1ul22qy1wxspy5cpkz504   \n",
       "1              z123dbgb0mqjfxbtz22ucjc5jvzcv3ykj   \n",
       "2          z12quxxp2vutflkxv04cihggzt2azl34pms0k   \n",
       "3              z12icv3ysqvlwth2c23eddlykyqut5z1h   \n",
       "4              z133stly3kete3tly22petvwdpmghrlli   \n",
       "..                                           ...   \n",
       "365  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA   \n",
       "366  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI   \n",
       "367  _2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs   \n",
       "368  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0   \n",
       "369  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA   \n",
       "\n",
       "                                 AUTHOR                        DATE  \\\n",
       "0                            dharma pal  2015-05-29T02:30:18.971000   \n",
       "1                         Tiza Arellano  2015-05-29T00:14:48.748000   \n",
       "2    Prìñçeśś Âliś Łøvê Dømíñø Mâđiś™ ﻿  2015-05-28T21:00:08.607000   \n",
       "3                         Eric Gonzalez  2015-05-28T20:47:12.193000   \n",
       "4                         Analena López  2015-05-28T17:08:29.827000   \n",
       "..                                  ...                         ...   \n",
       "365                        Katie Mettam  2013-07-13T13:27:39.441000   \n",
       "366                Sabina Pearson-Smith  2013-07-13T13:14:30.021000   \n",
       "367                       jeffrey jules  2013-07-13T12:09:31.188000   \n",
       "368                      Aishlin Maciel  2013-07-13T11:17:52.308000   \n",
       "369                         Latin Bosch  2013-07-12T22:33:27.916000   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "0                                           Nice song﻿      0  \n",
       "1                                        I love song ﻿      0  \n",
       "2                                        I love song ﻿      0  \n",
       "3    860,000,000 lets make it first female to reach...      0  \n",
       "4                        shakira is best for worldcup﻿      0  \n",
       "..                                                 ...    ...  \n",
       "365  I love this song because we sing it at Camp al...      0  \n",
       "366  I love this song for two reasons: 1.it is abou...      0  \n",
       "367                                                wow      0  \n",
       "368                            Shakira u are so wiredo      0  \n",
       "369                         Shakira is the best dancer      0  \n",
       "\n",
       "[370 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Youtube05-Shakira.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netooyer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supprimer les lignes \n",
    "df =df.dropna(subset=['CONTENT'])\n",
    "#Supprimer les doublons\n",
    "df = df.drop_duplicates(subset=['CONTENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction pour tokenizer, enlever les stopwords, lemmatizer ...etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour nettoyer, tokenizer, lemmatiser  et stemmer\n",
    "def process_text(text):\n",
    "    #convertir en minuscules\n",
    "    text = text.lower()\n",
    "    #Supprimer les caractères non alphabétiques\n",
    "    text=re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "    #Enlever html\n",
    "    text=re.sub(r'\\d+','',text)\n",
    "    #supprimer la ponctuation\n",
    "    text=re.sub(r'[^\\w\\s]', '', text)\n",
    "    #Tokenisation avec NLTK\n",
    "    tokens= word_tokenize(text)\n",
    "    #supprimer les stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if word not in stop_words]\n",
    "    #lemmatisation avec spacy\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized_tokens=[token.lemma_ for token in doc]\n",
    "    #stemming \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmer_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    #Rejoindre les tokens en une chaine\n",
    "    processed_text = ''.join(stemmer_tokens)\n",
    "    return  processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cerner la colonne à cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['CONTENT']\n",
    "#Appliquer la fonction à cette colonne\n",
    "for column in columns_to_clean:\n",
    "    df[column + '_processed']= df[column].apply(process_text)\n",
    "\n",
    "#Exporter les données clean en csv sans les indices\n",
    "df.to_csv(\"SHAKIRA1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =pd.read_csv(\"SHAKIRA1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>CONTENT_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>z12uujnj2sifvzvav04chpypvofvexpoggg</td>\n",
       "      <td>Sudheer Yadav</td>\n",
       "      <td>2015-05-28T10:28:25.133000</td>\n",
       "      <td>SEE SOME MORE SONG OPEN GOOGLE AND TYPE Shakir...</td>\n",
       "      <td>1</td>\n",
       "      <td>seesongopengoogltypeshakiraguruofmovi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>z13zjlpo2nbehxwf322gelhzwmqwgn1mt</td>\n",
       "      <td>Raafat saeed</td>\n",
       "      <td>2015-05-27T04:19:29.178000</td>\n",
       "      <td>Check out this playlist on YouTube:﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>checkplaylistyoutub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>z13uhhxp5nvig15yc04citszvtagwtmpqcc</td>\n",
       "      <td>Terry Short</td>\n",
       "      <td>2015-05-26T14:33:52.496000</td>\n",
       "      <td>Support the fight for your 4th amendment right...</td>\n",
       "      <td>1</td>\n",
       "      <td>supportfightthamendrightprivacihomestopnsaspia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>z13gv1bxbuytgjl3o23fdr5r3kaadbbm1</td>\n",
       "      <td>‫حلم الشباب‬‎</td>\n",
       "      <td>2015-05-25T23:42:49.533000</td>\n",
       "      <td>Check out this video on YouTube:﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>checkvideoyoutub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>z12bfraboyajftgbz04ccbkr3xjxfxyxsew</td>\n",
       "      <td>Abdullah Fawzi</td>\n",
       "      <td>2015-05-25T06:25:22.319000</td>\n",
       "      <td>coby this USL and past :&lt;br /&gt;&lt;a href=\"http://...</td>\n",
       "      <td>1</td>\n",
       "      <td>cobiuslpastbrhrefhttpadflyhttpadflyahmvtxbrdel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>z12xc3ly4x3uttmci22xff24nqqxwb0je04</td>\n",
       "      <td>Lisa Matthews</td>\n",
       "      <td>2013-07-17T13:56:03.233000</td>\n",
       "      <td>Check out this video on YouTube:&lt;br /&gt;&amp;quot;Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>checkvideoyoutubebrquotthitimeafricaquotonetra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>_2viQ_Qnc69GH3FQl348HonbRxpbmtsR5CUei0zkJog</td>\n",
       "      <td>Riley Rollins</td>\n",
       "      <td>2013-07-16T00:30:46.660000</td>\n",
       "      <td>O peoples of the earth, I have seen how you pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>peoplearthseeperformeveriformevilleisurceasrev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>_2viQ_Qnc6-qHJ_u9Yv84vj4yOAPLUL3ZibCc7b-vBI</td>\n",
       "      <td>FAHAD KHAN</td>\n",
       "      <td>2013-07-14T22:06:57.712000</td>\n",
       "      <td>I WILL NEVER FORGET THIS SONG IN MY LIFE LIKE ...</td>\n",
       "      <td>1</td>\n",
       "      <td>neverforgetsonglifelikecommenthearsonglikeyear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>_2viQ_Qnc6_HU65mTzCmXnjA-WLt7XqxqPj7EwAtlO0</td>\n",
       "      <td>ricky swaggz</td>\n",
       "      <td>2013-07-14T20:40:00.331000</td>\n",
       "      <td>********OMG Facebook is OLD! Check out  ------...</td>\n",
       "      <td>1</td>\n",
       "      <td>omgfacebookoldcheckgtswagfriendcommakethousand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>_2viQ_Qnc6_fgKR1W7-k1lbVURi8hVbMlQAMSOCSnyk</td>\n",
       "      <td>ThirdDegr3e</td>\n",
       "      <td>2013-07-13T20:48:22.967000</td>\n",
       "      <td>**CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...</td>\n",
       "      <td>1</td>\n",
       "      <td>checknewmixtapchecknewmixtapchecknewmixtapchec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMMENT_ID          AUTHOR  \\\n",
       "6            z12uujnj2sifvzvav04chpypvofvexpoggg   Sudheer Yadav   \n",
       "21             z13zjlpo2nbehxwf322gelhzwmqwgn1mt    Raafat saeed   \n",
       "29           z13uhhxp5nvig15yc04citszvtagwtmpqcc     Terry Short   \n",
       "34             z13gv1bxbuytgjl3o23fdr5r3kaadbbm1   ‫حلم الشباب‬‎   \n",
       "49           z12bfraboyajftgbz04ccbkr3xjxfxyxsew  Abdullah Fawzi   \n",
       "..                                           ...             ...   \n",
       "316          z12xc3ly4x3uttmci22xff24nqqxwb0je04   Lisa Matthews   \n",
       "318  _2viQ_Qnc69GH3FQl348HonbRxpbmtsR5CUei0zkJog   Riley Rollins   \n",
       "321  _2viQ_Qnc6-qHJ_u9Yv84vj4yOAPLUL3ZibCc7b-vBI      FAHAD KHAN   \n",
       "322  _2viQ_Qnc6_HU65mTzCmXnjA-WLt7XqxqPj7EwAtlO0    ricky swaggz   \n",
       "323  _2viQ_Qnc6_fgKR1W7-k1lbVURi8hVbMlQAMSOCSnyk     ThirdDegr3e   \n",
       "\n",
       "                           DATE  \\\n",
       "6    2015-05-28T10:28:25.133000   \n",
       "21   2015-05-27T04:19:29.178000   \n",
       "29   2015-05-26T14:33:52.496000   \n",
       "34   2015-05-25T23:42:49.533000   \n",
       "49   2015-05-25T06:25:22.319000   \n",
       "..                          ...   \n",
       "316  2013-07-17T13:56:03.233000   \n",
       "318  2013-07-16T00:30:46.660000   \n",
       "321  2013-07-14T22:06:57.712000   \n",
       "322  2013-07-14T20:40:00.331000   \n",
       "323  2013-07-13T20:48:22.967000   \n",
       "\n",
       "                                               CONTENT  CLASS  \\\n",
       "6    SEE SOME MORE SONG OPEN GOOGLE AND TYPE Shakir...      1   \n",
       "21                Check out this playlist on YouTube:﻿      1   \n",
       "29   Support the fight for your 4th amendment right...      1   \n",
       "34                   Check out this video on YouTube:﻿      1   \n",
       "49   coby this USL and past :<br /><a href=\"http://...      1   \n",
       "..                                                 ...    ...   \n",
       "316  Check out this video on YouTube:<br />&quot;Th...      1   \n",
       "318  O peoples of the earth, I have seen how you pe...      1   \n",
       "321  I WILL NEVER FORGET THIS SONG IN MY LIFE LIKE ...      1   \n",
       "322  ********OMG Facebook is OLD! Check out  ------...      1   \n",
       "323  **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...      1   \n",
       "\n",
       "                                     CONTENT_processed  \n",
       "6                seesongopengoogltypeshakiraguruofmovi  \n",
       "21                                 checkplaylistyoutub  \n",
       "29   supportfightthamendrightprivacihomestopnsaspia...  \n",
       "34                                    checkvideoyoutub  \n",
       "49   cobiuslpastbrhrefhttpadflyhttpadflyahmvtxbrdel...  \n",
       "..                                                 ...  \n",
       "316  checkvideoyoutubebrquotthitimeafricaquotonetra...  \n",
       "318  peoplearthseeperformeveriformevilleisurceasrev...  \n",
       "321     neverforgetsonglifelikecommenthearsonglikeyear  \n",
       "322  omgfacebookoldcheckgtswagfriendcommakethousand...  \n",
       "323  checknewmixtapchecknewmixtapchecknewmixtapchec...  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2[df2['CLASS']==1]\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des deux datasets (ancien et le clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 6)\n"
     ]
    }
   ],
   "source": [
    "shape_original = df.shape\n",
    "print(shape_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6)\n"
     ]
    }
   ],
   "source": [
    "shape_clean = df3.shape\n",
    "print(shape_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On remarque le nombre de lignes a diminué (sans les doublons et les nan)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **On peut calculer la distance pour avoir des information sur la similarité entre les data clean et brutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn\n",
    "from  sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration du model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(df2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTENT\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m model2 \u001b[38;5;241m=\u001b[39m Word2Vec(df2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTENT_processed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:491\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 491\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_vocab(\n\u001b[0;32m    492\u001b[0m     corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, progress_per\u001b[38;5;241m=\u001b[39mprogress_per, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count \u001b[38;5;241m=\u001b[39m corpus_count\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m total_words\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:586\u001b[0m, in \u001b[0;36mWord2Vec.scan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file:\n\u001b[0;32m    584\u001b[0m     corpus_iterable \u001b[38;5;241m=\u001b[39m LineSentence(corpus_file)\n\u001b[1;32m--> 586\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[0;32m    588\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollected \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types from a corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m raw words and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_vocab), total_words, corpus_count\n\u001b[0;32m    591\u001b[0m )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:569\u001b[0m, in \u001b[0;36mWord2Vec._scan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentence_no \u001b[38;5;241m%\u001b[39m progress_per \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    565\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: at sentence #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, processed \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m words, keeping \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    567\u001b[0m         sentence_no, total_words, \u001b[38;5;28mlen\u001b[39m(vocab),\n\u001b[0;32m    568\u001b[0m     )\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[0;32m    570\u001b[0m     vocab[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    571\u001b[0m total_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(df2[\"CONTENT\"],vector_size=100,)\n",
    "model2 = Word2Vec(df2[\"CONTENT_processed\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
